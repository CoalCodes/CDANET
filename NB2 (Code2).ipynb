{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3347069,"sourceType":"datasetVersion","datasetId":2020131}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision opencv-python numpy pandas matplotlib scikit-learn torchsummary kagglehub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport h5py\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport kagglehub\nimport glob\nimport random\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Configuration parameters\nCONFIG = {\n    'batch_size': 16,\n    'num_epochs': 15,  # Increased from 10 to 15\n    'base_lr': 5e-4,   # Changed from 1e-4\n    'weight_decay': 2e-5,  # Changed from 1e-4\n    'scheduler': 'onecycle',  # Options: 'cosine', 'onecycle'\n    'mixup_alpha': 0.2,  # Added mixup augmentation\n    'label_smoothing': 0.1,  # Added label smoothing\n    'dropout_rate': 0.4,  # Increased dropout\n    'model_variant': 'efficient_cbam',  # Options: 'efficient_basic', 'efficient_cbam', 'efficient_dual'\n    'num_classes': 3\n}\n\nprint(\"Training configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n\n# Download the dataset using kagglehub\nprint(\"Downloading dataset...\")\ndataset_path = kagglehub.dataset_download(\"ashkhagan/figshare-brain-tumor-dataset\")\nprint(\"Path to dataset files:\", dataset_path)\n\n# Data loading functions\ndef load_data():\n    \"\"\"Load and preprocess data from the downloaded directory\"\"\"\n    # First find all .mat files in the dataset directory and its subdirectories\n    mat_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file.endswith('.mat') and file != 'cvind.mat':\n                mat_files.append(os.path.join(root, file))\n   \n    print(f\"Found {len(mat_files)} .mat files\")\n   \n    # If we don't have enough files, exit\n    if len(mat_files) < 3000:\n        raise ValueError(f\"Expected ~3064 .mat files but found only {len(mat_files)}\")\n   \n    # Sort the files to ensure consistent order\n    mat_files.sort()\n   \n    # Prepare arrays for images and labels\n    img = np.zeros((len(mat_files), 224, 224))\n    lbl = []\n   \n    # Load each file\n    for i, file_path in enumerate(mat_files):\n        try:\n            with h5py.File(file_path, 'r') as f:\n                images = f['cjdata']\n                resized = cv2.resize(images['image'][:,:], (224, 224), interpolation=cv2.INTER_CUBIC)\n                x = np.asarray(resized)\n                x = (x - np.min(x)) / (np.max(x) - np.min(x))  # Normalization\n                x = x.reshape((1, 224, 224))\n                img[i] = x\n                lbl.append(int(images['label'][0]))\n               \n                if i % 500 == 0:\n                    print(f\"Processed {i} images\")\n        except Exception as e:\n            print(f\"Failed to load image at {file_path}: {e}\")\n   \n    # Find cvind.mat file\n    cvind_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file == 'cvind.mat':\n                cvind_files.append(os.path.join(root, file))\n   \n    if not cvind_files:\n        raise ValueError(\"Could not find cvind.mat file\")\n   \n    cvind_path = cvind_files[0]\n    print(f\"Found cvind.mat at: {cvind_path}\")\n   \n    # Load fold indices\n    with h5py.File(cvind_path, 'r') as f:\n        idx = np.array(f['cvind']).astype(np.int16).squeeze()\n   \n    return img, np.array(lbl), idx\n\n# Custom Dataset\nclass BrainTumorDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n       \n    def __len__(self):\n        return len(self.labels)\n   \n    def __getitem__(self, idx):\n        # Convert grayscale to RGB by repeating channel\n        image = self.images[idx]\n        image = np.repeat(image.reshape(224, 224, 1), 3, axis=2)\n        label = self.labels[idx] - 1  # Convert to 0-indexed\n       \n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n           \n        return image, label\n\n# Define transforms for training and validation with more aggressive augmentation\ndef get_transforms():\n    train_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.RandomRotation(30),  # Increased from 15 to 30\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Added scale\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Added color jitter\n        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),  # Added perspective transform\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomErasing(p=0.2)  # Added random erasing\n    ])\n   \n    val_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n   \n    return train_transform, val_transform\n\n# Get train and test splits\ndef get_train_test_data(images, labels, fold_indices, test_fold):\n    train_mask = fold_indices != test_fold\n    test_mask = fold_indices == test_fold\n   \n    train_images = images[train_mask]\n    train_labels = labels[train_mask]\n    test_images = images[test_mask]\n    test_labels = labels[test_mask]\n   \n    return (train_images, train_labels), (test_images, test_labels)\n\n# MixUp augmentation\ndef mixup_data(x, y, alpha=1.0):\n    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# Squeeze and Excitation Block\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n# CBAM: Convolutional Block Attention Module\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n       \n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n        )\n       \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return torch.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n       \n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n       \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return torch.sigmoid(out)\n\nclass CBAM(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n       \n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\n# Self-Attention Block\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n       \n    def forward(self, x):\n        batch_size, C, width, height = x.size()\n       \n        # Reshape for matrix multiplication\n        proj_query = self.query(x).view(batch_size, -1, width*height).permute(0, 2, 1)  # B X (W*H) X C\n        proj_key = self.key(x).view(batch_size, -1, width*height)  # B X C X (W*H)\n       \n        # Calculate attention map\n        attention = torch.bmm(proj_query, proj_key)  # B X (W*H) X (W*H)\n        attention = self.softmax(attention)\n       \n        # Apply attention to values\n        proj_value = self.value(x).view(batch_size, -1, width*height)  # B X C X (W*H)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B X C X (W*H)\n        out = out.view(batch_size, C, width, height)  # B X C X W X H\n       \n        # Add residual connection with learnable parameter gamma\n        out = self.gamma * out + x\n       \n        return out\n\n# Dual Path Block - combines features from multiple paths\nclass DualPathBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(DualPathBlock, self).__init__()\n       \n        # First path - standard convolution\n        self.conv_path = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n       \n        # Second path - depthwise separable convolution\n        self.dw_path = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n       \n        # Attention module\n        self.cbam = CBAM(out_channels)\n       \n        # Residual connection if dimensions change\n        self.downsample = None\n        if stride != 1 or in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n   \n    def forward(self, x):\n        residual = x\n       \n        # Process through both paths\n        out1 = self.conv_path(x)\n        out2 = self.dw_path(x)\n       \n        # Combine paths with element-wise addition\n        out = out1 + out2\n       \n        # Apply attention\n        out = self.cbam(out)\n       \n        # Apply residual connection\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n       \n        return out\n\n# Feature Pyramid Network (FPN) module\nclass FPN(nn.Module):\n    def __init__(self, channels):\n        super(FPN, self).__init__()\n        self.lateral_conv = nn.Conv2d(channels, 256, kernel_size=1)\n        self.output_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n       \n    def forward(self, x):\n        lateral = self.lateral_conv(x)\n        output = self.output_conv(lateral)\n        return output\n\n# Main model with EfficientNet backbone and attention - Modified to support different variants\nclass EnhancedEfficientNetClassifier(nn.Module):\n    def __init__(self, model_variant='efficient_basic', num_classes=3, dropout_rate=0.3):\n        super(EnhancedEfficientNetClassifier, self).__init__()\n       \n        # Load pretrained EfficientNet and remove final classifier\n        # Using B4 instead of B3 for higher capacity\n        efficient_net = models.efficientnet_b4(weights=\"IMAGENET1K_V1\")\n        self.features = nn.Sequential(*list(efficient_net.children())[:-1])\n       \n        # Get the output feature dimension\n        feature_dim = efficient_net._modules['classifier'][1].in_features\n        split_dim = feature_dim // 2\n       \n        self.model_variant = model_variant\n       \n        if model_variant == 'efficient_basic':\n            # Basic variant with Self-Attention and SE blocks\n            self.attention1 = SelfAttention(split_dim)\n            self.attention2 = SelfAttention(split_dim)\n            self.se1 = SEBlock(split_dim)\n            self.se2 = SEBlock(split_dim)\n           \n        elif model_variant == 'efficient_cbam':\n            # Enhanced variant with CBAM\n            self.cbam1 = CBAM(split_dim)\n            self.cbam2 = CBAM(split_dim)\n           \n        elif model_variant == 'efficient_dual':\n            # Dual path variant\n            self.dual_path1 = DualPathBlock(split_dim, split_dim)\n            self.dual_path2 = DualPathBlock(split_dim, split_dim)\n           \n        # FPN modules for multi-scale feature extraction\n        self.fpn1 = FPN(split_dim)\n        self.fpn2 = FPN(split_dim)\n       \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n       \n        # Additional features: Global Context Block\n        self.gcb = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(64, 512, kernel_size=1),\n            nn.Sigmoid()\n        )\n       \n        # Fully connected layers with improved regularization\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),  # Increased dropout\n            nn.Linear(512, 256),  # Added an extra FC layer\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate/2),  # Lower dropout in the last layer\n            nn.Linear(256, num_classes)\n        )\n   \n    def forward(self, x):\n        # Extract features using EfficientNet backbone\n        features = self.features(x)\n       \n        # Split features into two parts\n        features1, features2 = torch.split(features, features.size(1)//2, dim=1)\n       \n        # Apply attention based on model variant\n        if self.model_variant == 'efficient_basic':\n            features1 = self.attention1(features1)\n            features1 = self.se1(features1)\n           \n            features2 = self.attention2(features2)\n            features2 = self.se2(features2)\n           \n        elif self.model_variant == 'efficient_cbam':\n            features1 = self.cbam1(features1)\n            features2 = self.cbam2(features2)\n           \n        elif self.model_variant == 'efficient_dual':\n            features1 = self.dual_path1(features1)\n            features2 = self.dual_path2(features2)\n       \n        # Apply FPN for multi-scale feature enhancement\n        features1 = self.fpn1(features1)\n        features2 = self.fpn2(features2)\n       \n        # Concatenate features\n        combined_features = torch.cat([features1, features2], dim=1)\n       \n        # Apply global context\n        context = self.gcb(combined_features)\n        combined_features = combined_features * context\n       \n        # Global pooling\n        pooled = self.global_pool(combined_features)\n        pooled = pooled.view(pooled.size(0), -1)\n       \n        # Classification\n        output = self.classifier(pooled)\n       \n        return output\n\n# Training function with mixup and label smoothing\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, mixup_alpha=0.0):\n    best_val_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n   \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n       \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        total_samples = 0\n       \n        for inputs, labels in train_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n           \n            # Apply mixup if alpha > 0\n            if mixup_alpha > 0:\n                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)\n                use_mixup = True\n            else:\n                use_mixup = False\n           \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n           \n            # Forward pass\n            outputs = model(inputs)\n           \n            # Calculate loss with or without mixup\n            if use_mixup:\n                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                # For accuracy calculation with mixup, we use the dominant label\n                _, preds = torch.max(outputs, 1)\n                running_corrects += (lam * torch.sum(preds == labels_a) +\n                                   (1 - lam) * torch.sum(preds == labels_b)).float()\n            else:\n                loss = criterion(outputs, labels)\n                _, preds = torch.max(outputs, 1)\n                running_corrects += torch.sum(preds == labels.data)\n           \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n           \n            # Statistics\n            batch_size = inputs.size(0)\n            running_loss += loss.item() * batch_size\n            total_samples += batch_size\n       \n        # Step scheduler if it's not OneCycleLR (which steps per batch)\n        if CONFIG['scheduler'] != 'onecycle':\n            scheduler.step()\n       \n        epoch_loss = running_loss / total_samples\n        epoch_acc = running_corrects.double() / total_samples\n       \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.cpu().numpy())\n       \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n       \n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        val_running_corrects = 0\n        val_total_samples = 0\n       \n        # No gradient calculation needed for validation\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n               \n                # Forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n               \n                # Statistics\n                batch_size = inputs.size(0)\n                val_running_loss += loss.item() * batch_size\n                val_running_corrects += torch.sum(preds == labels.data)\n                val_total_samples += batch_size\n       \n        val_epoch_loss = val_running_loss / val_total_samples\n        val_epoch_acc = val_running_corrects.double() / val_total_samples\n       \n        history['val_loss'].append(val_epoch_loss)\n        history['val_acc'].append(val_epoch_acc.cpu().numpy())\n       \n        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n       \n        # Save best model\n        if val_epoch_acc > best_val_acc:\n            best_val_acc = val_epoch_acc\n            torch.save(model.state_dict(), f'best_{CONFIG[\"model_variant\"]}_model.pth')\n            print(f'New best model saved with accuracy: {val_epoch_acc:.4f}')\n       \n        print()\n   \n    # Load best model weights\n    model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n    return model, history\n\n# Evaluation function with advanced metrics\ndef evaluate_model(model, test_loader, num_classes=3):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n   \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = F.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n           \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n   \n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n   \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n   \n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n   \n    # Classification Report\n    report = classification_report(all_labels, all_preds)\n   \n    # ROC Curve and AUC\n    # Binarize the labels for ROC calculation\n    labels_bin = label_binarize(all_labels, classes=range(num_classes))\n   \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(num_classes):\n        fpr[i], tpr[i], _ = roc_curve(labels_bin[:, i], all_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n   \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_bin.ravel(), all_probs.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n   \n    # Compute macro-average ROC curve and ROC area\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n   \n    # Then interpolate all ROC curves at these points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(num_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n   \n    # Finally average it and compute AUC\n    mean_tpr /= num_classes\n   \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n   \n    # Plot ROC Curves\n    plt.figure(figsize=(12, 8))\n   \n    # Plot micro-average ROC curve\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n             color='deeppink', linestyle=':', linewidth=4)\n   \n    # Plot macro-average ROC curve\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=f'Macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n             color='navy', linestyle=':', linewidth=4)\n   \n    # Plot ROC curves for all classes\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n   \n    for i, color, name in zip(range(num_classes), colors, class_names):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label=f'ROC curve of {name} (area = {roc_auc[i]:.2f})')\n   \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve - {CONFIG[\"model_variant\"]}')\n    plt.legend(loc=\"lower right\")\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_roc_curve.png')\n    plt.close()\n   \n    # Plot Confusion Matrix\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'Confusion Matrix - {CONFIG[\"model_variant\"]}')\n    plt.colorbar()\n   \n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n   \n    # Add text annotations to the confusion matrix\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], 'd'),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n   \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_confusion_matrix.png')\n    plt.close()\n   \n    # Return metrics and predictions\n    return {\n        'accuracy': accuracy,\n        'confusion_matrix': cm,\n        'classification_report': report,\n        'roc_auc': roc_auc,\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs\n    }\n\n# Function to test with a random image\ndef test_random_image(model, images, labels, transform=None):\n    \"\"\"\n    Test the model with a random image from the dataset\n   \n    Args:\n        model: Trained model\n        images: Image data\n        labels: Labels\n        transform: Image transform function\n       \n    Returns:\n        prediction: The model's prediction\n    \"\"\"\n    # Choose a random image\n    idx = random.randint(0, len(images) - 1)\n    img = images[idx]\n    true_label = labels[idx] - 1  # Convert totrue_label = labels[idx] - 1  # Convert to 0-indexed\n   \n    # Preprocess the image\n    img = np.repeat(img.reshape(224, 224, 1), 3, axis=2)\n   \n    # Apply transform if provided\n    if transform:\n        img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n    else:\n        img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n   \n    # Get model prediction\n    model.eval()\n    with torch.no_grad():\n        output = model(img_tensor)\n        probs = F.softmax(output, dim=1)\n        _, pred = torch.max(output, 1)\n   \n    # Display the image and prediction\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n    plt.figure(figsize=(6, 6))\n    plt.imshow(img, cmap='gray')\n    plt.title(f'True: {class_names[true_label]}, Pred: {class_names[pred.item()]}\\nConfidence: {probs[0][pred.item()]:.4f}')\n    plt.axis('off')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_random_test.png')\n    plt.close()\n   \n    print(f\"Random image test - True: {class_names[true_label]}, Predicted: {class_names[pred.item()]}\")\n    print(f\"Confidence scores: {probs[0].cpu().numpy()}\")\n   \n    return {\n        'image_idx': idx,\n        'true_label': true_label,\n        'predicted_label': pred.item(),\n        'confidence': probs[0][pred.item()].item(),\n        'all_probs': probs[0].cpu().numpy()\n    }\n\n# Main execution function\ndef run_experiment(fold_index):\n    print(f\"\\n{'='*20} RUNNING FOLD {fold_index} {'='*20}\\n\")\n   \n    # Load data\n    images, labels, fold_indices = load_data()\n   \n    # Get train and test data for this fold\n    (train_images, train_labels), (test_images, test_labels) = get_train_test_data(\n        images, labels, fold_indices, fold_index)\n   \n    # Create datasets with transforms\n    train_transform, val_transform = get_transforms()\n   \n    train_dataset = BrainTumorDataset(train_images, train_labels, transform=train_transform)\n    test_dataset = BrainTumorDataset(test_images, test_labels, transform=val_transform)\n   \n    # Create data loaders with larger batch size\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['batch_size'],\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True  # Add pin_memory for faster data transfer to GPU\n    )\n   \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=CONFIG['batch_size'],\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n   \n    # Initialize model based on selected variant\n    model = EnhancedEfficientNetClassifier(\n        model_variant=CONFIG['model_variant'],\n        num_classes=CONFIG['num_classes'],\n        dropout_rate=CONFIG['dropout_rate']\n    )\n    model = model.to(device)\n   \n    # Loss function with label smoothing\n    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n   \n    # Optimizer with weight decay\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=CONFIG['base_lr'],\n        weight_decay=CONFIG['weight_decay']\n    )\n   \n    # Set scheduler based on configuration\n    if CONFIG['scheduler'] == 'cosine':\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=CONFIG['num_epochs'],\n            eta_min=CONFIG['base_lr'] / 100\n        )\n    elif CONFIG['scheduler'] == 'onecycle':\n        # OneCycle learning rate scheduler\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=CONFIG['base_lr'] * 10,  # Peak LR is 10x the base LR\n            steps_per_epoch=len(train_loader),\n            epochs=CONFIG['num_epochs'],\n            pct_start=0.3,  # Spend 30% of time increasing LR\n            div_factor=25.0,  # Initial LR = max_lr/25\n            final_div_factor=10000.0  # Final LR = initial_lr/10000\n        )\n   \n    # Train model with specified epochs\n    start_time = time.time()\n    model, history = train_model(\n        model,\n        train_loader,\n        test_loader,\n        criterion,\n        optimizer,\n        scheduler,\n        num_epochs=CONFIG['num_epochs'],\n        mixup_alpha=CONFIG['mixup_alpha']\n    )\n    end_time = time.time()\n   \n    # Evaluate model with advanced metrics\n    metrics = evaluate_model(model, test_loader, num_classes=CONFIG['num_classes'])\n    accuracy = metrics['accuracy']\n   \n    print(f\"Fold {fold_index} accuracy: {accuracy:.4f}\")\n    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n    print(\"\\nClassification Report:\")\n    print(metrics['classification_report'])\n   \n    # Get ROC AUC scores\n    print(\"\\nROC AUC Scores:\")\n    for i in range(CONFIG['num_classes']):\n        print(f\"Class {i}: {metrics['roc_auc'][i]:.4f}\")\n    print(f\"Micro-average: {metrics['roc_auc']['micro']:.4f}\")\n    print(f\"Macro-average: {metrics['roc_auc']['macro']:.4f}\")\n   \n    # Plot loss curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Loss Curves for Fold {fold_index} - {CONFIG[\"model_variant\"]}')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_loss_curve_fold_{fold_index}.png')\n    plt.close()\n   \n    # Plot accuracy curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_acc'], label='Train Acc')\n    plt.plot(history['val_acc'], label='Val Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title(f'Accuracy Curves for Fold {fold_index} - {CONFIG[\"model_variant\"]}')\n    plt.savefig(f'{CONFIG[\"model_variant\"]}_acc_curve_fold_{fold_index}.png')\n    plt.close()\n   \n    # Test with a random image\n    random_test_results = test_random_image(model, images, labels, transform=val_transform)\n   \n    # Save model and results\n    torch.save(model.state_dict(), f'{CONFIG[\"model_variant\"]}_model_fold_{fold_index}.pth')\n    np.save(f'{CONFIG[\"model_variant\"]}_results_fold_{fold_index}.npy', metrics)\n   \n    return metrics\n\n# Run for all folds\nif __name__ == \"__main__\":\n    results_all = {}\n   \n    # Run for all 5 folds\n    for fold in range(1, 6):\n        try:\n            metrics = run_experiment(fold)\n            results_all[fold] = metrics\n        except Exception as e:\n            print(f\"Error in fold {fold}: {e}\")\n   \n    # Calculate and print average accuracy across all folds\n    accuracies = [results_all[fold]['accuracy'] for fold in results_all if fold in results_all]\n    if accuracies:\n        avg_accuracy = np.mean(accuracies)\n        print(f\"\\nAverage accuracy across all folds: {avg_accuracy:.4f}\")\n       \n        # Calculate average AUC across all folds\n        avg_auc_micro = np.mean([results_all[fold]['roc_auc']['micro'] for fold in results_all if fold in results_all])\n        avg_auc_macro = np.mean([results_all[fold]['roc_auc']['macro'] for fold in results_all if fold in results_all])\n        print(f\"Average micro-average AUC: {avg_auc_micro:.4f}\")\n        print(f\"Average macro-average AUC: {avg_auc_macro:.4f}\")\n       \n        # Model and configuration summary\n        print(\"\\nModel Configuration Summary:\")\n        print(f\"  Model variant: {CONFIG['model_variant']}\")\n        print(f\"  Batch size: {CONFIG['batch_size']}\")\n        print(f\"  Epochs: {CONFIG['num_epochs']}\")\n        print(f\"  Learning rate: {CONFIG['base_lr']}\")\n        print(f\"  Weight decay: {CONFIG['weight_decay']}\")\n        print(f\"  Scheduler: {CONFIG['scheduler']}\")\n        print(f\"  MixUp alpha: {CONFIG['mixup_alpha']}\")\n        print(f\"  Label smoothing: {CONFIG['label_smoothing']}\")\n        print(f\"  Dropout rate: {CONFIG['dropout_rate']}\")\n       \n    # Visualize learning curves across all folds\n    if results_all:\n        # Plot average accuracy across folds\n        plt.figure(figsize=(12, 6))\n        for fold in results_all:\n            plt.plot(results_all[fold]['val_acc'], label=f'Fold {fold}')\n        plt.xlabel('Epoch')\n        plt.ylabel('Validation Accuracy')\n        plt.title(f'Validation Accuracy Across Folds - {CONFIG[\"model_variant\"]}')\n        plt.legend()\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.savefig(f'{CONFIG[\"model_variant\"]}_all_folds_accuracy.png')\n        plt.close()\n       \n        # Generate final summary visualization of class-wise metrics\n        class_names = ['Meningioma', 'Glioma', 'Pituitary']\n        class_aucs = []\n        for i in range(CONFIG['num_classes']):\n            class_auc = np.mean([results_all[fold]['roc_auc'][i] for fold in results_all if fold in results_all])\n            class_aucs.append(class_auc)\n       \n        plt.figure(figsize=(10, 6))\n        plt.bar(class_names, class_aucs, color=['skyblue', 'lightgreen', 'salmon'])\n        plt.ylabel('Average AUC')\n        plt.title(f'Average AUC by Class - {CONFIG[\"model_variant\"]}')\n        plt.ylim(0.8, 1.0)  # Adjust as needed\n        for i, v in enumerate(class_aucs):\n            plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n        plt.savefig(f'{CONFIG[\"model_variant\"]}_class_aucs.png')\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:02:23.690381Z","iopub.execute_input":"2025-05-01T17:02:23.690574Z","iopub.status.idle":"2025-05-01T17:48:23.515740Z","shell.execute_reply.started":"2025-05-01T17:02:23.690557Z","shell.execute_reply":"2025-05-01T17:48:23.514626Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining configuration:\n  batch_size: 16\n  num_epochs: 15\n  base_lr: 0.0005\n  weight_decay: 2e-05\n  scheduler: onecycle\n  mixup_alpha: 0.2\n  label_smoothing: 0.1\n  dropout_rate: 0.4\n  model_variant: efficient_cbam\n  num_classes: 3\nDownloading dataset...\nPath to dataset files: /kaggle/input/figshare-brain-tumor-dataset\n\n==================== RUNNING FOLD 1 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:00<00:00, 187MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n----------\nTrain Loss: 0.8524 Acc: 0.6502\nVal Loss: 0.5665 Acc: 0.8450\nNew best model saved with accuracy: 0.8450\n\nEpoch 2/15\n----------\nTrain Loss: 0.7178 Acc: 0.7592\nVal Loss: 0.4905 Acc: 0.8985\nNew best model saved with accuracy: 0.8985\n\nEpoch 3/15\n----------\nTrain Loss: 0.6517 Acc: 0.8084\nVal Loss: 0.4429 Acc: 0.9207\nNew best model saved with accuracy: 0.9207\n\nEpoch 4/15\n----------\nTrain Loss: 0.6410 Acc: 0.8180\nVal Loss: 0.4472 Acc: 0.9225\nNew best model saved with accuracy: 0.9225\n\nEpoch 5/15\n----------\nTrain Loss: 0.6327 Acc: 0.8201\nVal Loss: 0.4410 Acc: 0.9207\n\nEpoch 6/15\n----------\nTrain Loss: 0.6011 Acc: 0.8440\nVal Loss: 0.4235 Acc: 0.9207\n\nEpoch 7/15\n----------\nTrain Loss: 0.5898 Acc: 0.8516\nVal Loss: 0.4014 Acc: 0.9483\nNew best model saved with accuracy: 0.9483\n\nEpoch 8/15\n----------\nTrain Loss: 0.6075 Acc: 0.8425\nVal Loss: 0.3908 Acc: 0.9465\n\nEpoch 9/15\n----------\nTrain Loss: 0.5618 Acc: 0.8664\nVal Loss: 0.4145 Acc: 0.9373\n\nEpoch 10/15\n----------\nTrain Loss: 0.5813 Acc: 0.8625\nVal Loss: 0.3734 Acc: 0.9576\nNew best model saved with accuracy: 0.9576\n\nEpoch 11/15\n----------\nTrain Loss: 0.5798 Acc: 0.8596\nVal Loss: 0.3813 Acc: 0.9594\nNew best model saved with accuracy: 0.9594\n\nEpoch 12/15\n----------\nTrain Loss: 0.5391 Acc: 0.8848\nVal Loss: 0.3929 Acc: 0.9465\n\nEpoch 13/15\n----------\nTrain Loss: 0.5684 Acc: 0.8675\nVal Loss: 0.3637 Acc: 0.9686\nNew best model saved with accuracy: 0.9686\n\nEpoch 14/15\n----------\nTrain Loss: 0.5767 Acc: 0.8585\nVal Loss: 0.3803 Acc: 0.9520\n\nEpoch 15/15\n----------\nTrain Loss: 0.5459 Acc: 0.8731\nVal Loss: 0.3564 Acc: 0.9649\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 accuracy: 0.9686\nTraining time: 610.73 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.94      0.93       113\n           1       0.99      0.97      0.98       288\n           2       0.97      1.00      0.99       141\n\n    accuracy                           0.97       542\n   macro avg       0.96      0.97      0.96       542\nweighted avg       0.97      0.97      0.97       542\n\n\nROC AUC Scores:\nClass 0: 0.9946\nClass 1: 0.9986\nClass 2: 0.9994\nMicro-average: 0.9979\nMacro-average: 0.9979\nRandom image test - True: Meningioma, Predicted: Meningioma\nConfidence scores: [0.72648036 0.2397099  0.0338097 ]\n\n==================== RUNNING FOLD 2 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nError in fold 2: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n\n==================== RUNNING FOLD 3 ====================\n\nFound 3064 .mat files\nProcessed 0 images\nProcessed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8472 Acc: 0.6590\nVal Loss: 0.5645 Acc: 0.8671\nNew best model saved with accuracy: 0.8671\n\nEpoch 2/15\n----------\nTrain Loss: 0.7127 Acc: 0.7597\nVal Loss: 0.4813 Acc: 0.9091\nNew best model saved with accuracy: 0.9091\n\nEpoch 3/15\n----------\nTrain Loss: 0.6680 Acc: 0.7980\nVal Loss: 0.4512 Acc: 0.9406\nNew best model saved with accuracy: 0.9406\n\nEpoch 4/15\n----------\nTrain Loss: 0.6447 Acc: 0.8108\nVal Loss: 0.4085 Acc: 0.9493\nNew best model saved with accuracy: 0.9493\n\nEpoch 5/15\n----------\nTrain Loss: 0.6227 Acc: 0.8254\nVal Loss: 0.4058 Acc: 0.9493\n\nEpoch 6/15\n----------\nTrain Loss: 0.6064 Acc: 0.8374\nVal Loss: 0.3942 Acc: 0.9580\nNew best model saved with accuracy: 0.9580\n\nEpoch 7/15\n----------\nTrain Loss: 0.5981 Acc: 0.8456\nVal Loss: 0.4064 Acc: 0.9510\n\nEpoch 8/15\n----------\nTrain Loss: 0.5668 Acc: 0.8634\nVal Loss: 0.3916 Acc: 0.9580\n\nEpoch 9/15\n----------\nTrain Loss: 0.5616 Acc: 0.8659\nVal Loss: 0.3702 Acc: 0.9615\nNew best model saved with accuracy: 0.9615\n\nEpoch 10/15\n----------\nTrain Loss: 0.5582 Acc: 0.8672\nVal Loss: 0.3647 Acc: 0.9615\n\nEpoch 11/15\n----------\nTrain Loss: 0.5745 Acc: 0.8597\nVal Loss: 0.3770 Acc: 0.9580\n\nEpoch 12/15\n----------\nTrain Loss: 0.5655 Acc: 0.8720\nVal Loss: 0.3679 Acc: 0.9598\n\nEpoch 13/15\n----------\nTrain Loss: 0.5536 Acc: 0.8818\nVal Loss: 0.3891 Acc: 0.9563\n\nEpoch 14/15\n----------\nTrain Loss: 0.5623 Acc: 0.8677\nVal Loss: 0.3836 Acc: 0.9580\n\nEpoch 15/15\n----------\nTrain Loss: 0.5512 Acc: 0.8758\nVal Loss: 0.3849 Acc: 0.9545\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 accuracy: 0.9615\nTraining time: 611.12 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.91      0.90        99\n           1       0.97      0.98      0.97       267\n           2       0.99      0.96      0.97       206\n\n    accuracy                           0.96       572\n   macro avg       0.95      0.95      0.95       572\nweighted avg       0.96      0.96      0.96       572\n\n\nROC AUC Scores:\nClass 0: 0.9913\nClass 1: 0.9964\nClass 2: 0.9990\nMicro-average: 0.9964\nMacro-average: 0.9960\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.08611514 0.05901101 0.8548739 ]\n\n==================== RUNNING FOLD 4 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8509 Acc: 0.6553\nVal Loss: 0.6091 Acc: 0.8535\nNew best model saved with accuracy: 0.8535\n\nEpoch 2/15\n----------\nTrain Loss: 0.7163 Acc: 0.7583\nVal Loss: 0.5229 Acc: 0.8822\nNew best model saved with accuracy: 0.8822\n\nEpoch 3/15\n----------\nTrain Loss: 0.6532 Acc: 0.8063\nVal Loss: 0.4652 Acc: 0.9204\nNew best model saved with accuracy: 0.9204\n\nEpoch 4/15\n----------\nTrain Loss: 0.6149 Acc: 0.8352\nVal Loss: 0.4361 Acc: 0.9299\nNew best model saved with accuracy: 0.9299\n\nEpoch 5/15\n----------\nTrain Loss: 0.6193 Acc: 0.8314\nVal Loss: 0.4259 Acc: 0.9363\nNew best model saved with accuracy: 0.9363\n\nEpoch 6/15\n----------\nTrain Loss: 0.6026 Acc: 0.8459\nVal Loss: 0.4210 Acc: 0.9411\nNew best model saved with accuracy: 0.9411\n\nEpoch 7/15\n----------\nTrain Loss: 0.5853 Acc: 0.8534\nVal Loss: 0.4118 Acc: 0.9459\nNew best model saved with accuracy: 0.9459\n\nEpoch 8/15\n----------\nTrain Loss: 0.6054 Acc: 0.8448\nVal Loss: 0.4320 Acc: 0.9331\n\nEpoch 9/15\n----------\nTrain Loss: 0.5757 Acc: 0.8723\nVal Loss: 0.3820 Acc: 0.9554\nNew best model saved with accuracy: 0.9554\n\nEpoch 10/15\n----------\nTrain Loss: 0.5411 Acc: 0.8840\nVal Loss: 0.3879 Acc: 0.9506\n\nEpoch 11/15\n----------\nTrain Loss: 0.5714 Acc: 0.8636\nVal Loss: 0.4015 Acc: 0.9618\nNew best model saved with accuracy: 0.9618\n\nEpoch 12/15\n----------\nTrain Loss: 0.5664 Acc: 0.8642\nVal Loss: 0.3832 Acc: 0.9634\nNew best model saved with accuracy: 0.9634\n\nEpoch 13/15\n----------\nTrain Loss: 0.5672 Acc: 0.8630\nVal Loss: 0.3693 Acc: 0.9570\n\nEpoch 14/15\n----------\nTrain Loss: 0.5631 Acc: 0.8687\nVal Loss: 0.3819 Acc: 0.9697\nNew best model saved with accuracy: 0.9697\n\nEpoch 15/15\n----------\nTrain Loss: 0.5485 Acc: 0.8820\nVal Loss: 0.3580 Acc: 0.9729\nNew best model saved with accuracy: 0.9729\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 accuracy: 0.9729\nTraining time: 607.98 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       168\n           1       0.98      0.98      0.98       287\n           2       0.97      0.97      0.97       173\n\n    accuracy                           0.97       628\n   macro avg       0.97      0.97      0.97       628\nweighted avg       0.97      0.97      0.97       628\n\n\nROC AUC Scores:\nClass 0: 0.9975\nClass 1: 0.9988\nClass 2: 0.9988\nMicro-average: 0.9984\nMacro-average: 0.9985\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.07824273 0.04280282 0.8789544 ]\n\n==================== RUNNING FOLD 5 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:93: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/15\n----------\nTrain Loss: 0.8281 Acc: 0.6729\nVal Loss: 0.5819 Acc: 0.8554\nNew best model saved with accuracy: 0.8554\n\nEpoch 2/15\n----------\nTrain Loss: 0.7144 Acc: 0.7590\nVal Loss: 0.5155 Acc: 0.8787\nNew best model saved with accuracy: 0.8787\n\nEpoch 3/15\n----------\nTrain Loss: 0.6549 Acc: 0.8035\nVal Loss: 0.4451 Acc: 0.9160\nNew best model saved with accuracy: 0.9160\n\nEpoch 4/15\n----------\nTrain Loss: 0.6380 Acc: 0.8211\nVal Loss: 0.4333 Acc: 0.9300\nNew best model saved with accuracy: 0.9300\n\nEpoch 5/15\n----------\nTrain Loss: 0.6243 Acc: 0.8340\nVal Loss: 0.4299 Acc: 0.9300\n\nEpoch 6/15\n----------\nTrain Loss: 0.6443 Acc: 0.8251\nVal Loss: 0.4390 Acc: 0.9347\nNew best model saved with accuracy: 0.9347\n\nEpoch 7/15\n----------\nTrain Loss: 0.6060 Acc: 0.8417\nVal Loss: 0.4356 Acc: 0.9222\n\nEpoch 8/15\n----------\nTrain Loss: 0.5870 Acc: 0.8536\nVal Loss: 0.3986 Acc: 0.9533\nNew best model saved with accuracy: 0.9533\n\nEpoch 9/15\n----------\nTrain Loss: 0.6011 Acc: 0.8449\nVal Loss: 0.4002 Acc: 0.9518\n\nEpoch 10/15\n----------\nTrain Loss: 0.5980 Acc: 0.8484\nVal Loss: 0.3973 Acc: 0.9456\n\nEpoch 11/15\n----------\nTrain Loss: 0.5813 Acc: 0.8601\nVal Loss: 0.3904 Acc: 0.9502\n\nEpoch 12/15\n----------\nTrain Loss: 0.5706 Acc: 0.8691\nVal Loss: 0.3882 Acc: 0.9533\n\nEpoch 13/15\n----------\nTrain Loss: 0.5402 Acc: 0.8853\nVal Loss: 0.3742 Acc: 0.9596\nNew best model saved with accuracy: 0.9596\n\nEpoch 14/15\n----------\nTrain Loss: 0.5642 Acc: 0.8662\nVal Loss: 0.3797 Acc: 0.9549\n\nEpoch 15/15\n----------\nTrain Loss: 0.5470 Acc: 0.8746\nVal Loss: 0.3986 Acc: 0.9393\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1625073196.py:564: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{CONFIG[\"model_variant\"]}_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 accuracy: 0.9596\nTraining time: 603.78 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.94      0.91       144\n           1       0.99      0.96      0.97       296\n           2       0.97      0.98      0.98       203\n\n    accuracy                           0.96       643\n   macro avg       0.95      0.96      0.95       643\nweighted avg       0.96      0.96      0.96       643\n\n\nROC AUC Scores:\nClass 0: 0.9888\nClass 1: 0.9985\nClass 2: 0.9942\nMicro-average: 0.9945\nMacro-average: 0.9945\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.03249943 0.03405317 0.9334475 ]\n\nAverage accuracy across all folds: 0.9657\nAverage micro-average AUC: 0.9968\nAverage macro-average AUC: 0.9967\n\nModel Configuration Summary:\n  Model variant: efficient_cbam\n  Batch size: 16\n  Epochs: 15\n  Learning rate: 0.0005\n  Weight decay: 2e-05\n  Scheduler: onecycle\n  MixUp alpha: 0.2\n  Label smoothing: 0.1\n  Dropout rate: 0.4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1625073196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Fold {fold}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'val_acc'"],"ename":"KeyError","evalue":"'val_acc'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 0 Axes>"},"metadata":{}}],"execution_count":1}]}