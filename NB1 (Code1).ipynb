{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3347069,"sourceType":"datasetVersion","datasetId":2020131}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision opencv-python numpy pandas matplotlib scikit-learn torchsummary kagglehub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport h5py\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport kagglehub\nimport glob\nimport random\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Download the dataset using kagglehub\nprint(\"Downloading dataset...\")\ndataset_path = kagglehub.dataset_download(\"ashkhagan/figshare-brain-tumor-dataset\")\nprint(\"Path to dataset files:\", dataset_path)\n\n# Data loading functions\ndef load_data():\n    \"\"\"Load and preprocess data from the downloaded directory\"\"\"\n    # First find all .mat files in the dataset directory and its subdirectories\n    mat_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file.endswith('.mat') and file != 'cvind.mat':\n                mat_files.append(os.path.join(root, file))\n   \n    print(f\"Found {len(mat_files)} .mat files\")\n   \n    # If we don't have enough files, exit\n    if len(mat_files) < 3000:\n        raise ValueError(f\"Expected ~3064 .mat files but found only {len(mat_files)}\")\n   \n    # Sort the files to ensure consistent order\n    mat_files.sort()\n   \n    # Prepare arrays for images and labels\n    img = np.zeros((len(mat_files), 224, 224))\n    lbl = []\n   \n    # Load each file\n    for i, file_path in enumerate(mat_files):\n        try:\n            with h5py.File(file_path, 'r') as f:\n                images = f['cjdata']\n                resized = cv2.resize(images['image'][:,:], (224, 224), interpolation=cv2.INTER_CUBIC)\n                x = np.asarray(resized)\n                x = (x - np.min(x)) / (np.max(x) - np.min(x))  # Normalization\n                x = x.reshape((1, 224, 224))\n                img[i] = x\n                lbl.append(int(images['label'][0]))\n               \n                if i % 500 == 0:\n                    print(f\"Processed {i} images\")\n        except Exception as e:\n            print(f\"Failed to load image at {file_path}: {e}\")\n   \n    # Find cvind.mat file\n    cvind_files = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file == 'cvind.mat':\n                cvind_files.append(os.path.join(root, file))\n   \n    if not cvind_files:\n        raise ValueError(\"Could not find cvind.mat file\")\n   \n    cvind_path = cvind_files[0]\n    print(f\"Found cvind.mat at: {cvind_path}\")                                                              # Load fold indices\n    with h5py.File(cvind_path, 'r') as f:\n        idx = np.array(f['cvind']).astype(np.int16).squeeze()\n   \n    return img, np.array(lbl), idx\n\n# Custom Dataset\nclass BrainTumorDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n       \n    def __len__(self):\n        return len(self.labels)\n   \n    def __getitem__(self, idx):\n        # Convert grayscale to RGB by repeating channel\n        image = self.images[idx]\n        image = np.repeat(image.reshape(224, 224, 1), 3, axis=2)\n        label = self.labels[idx] - 1  # Convert to 0-indexed\n       \n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n           \n        return image, label\n\n# Define transforms for training and validation\ndef get_transforms():\n    train_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(15),\n        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n   \n    val_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n   \n    return train_transform, val_transform\n\n# Get train and test splits\ndef get_train_test_data(images, labels, fold_indices, test_fold):\n    train_mask = fold_indices != test_fold\n    test_mask = fold_indices == test_fold\n   \n    train_images = images[train_mask]\n    train_labels = labels[train_mask]\n    test_images = images[test_mask]\n    test_labels = labels[test_mask]\n   \n    return (train_images, train_labels), (test_images, test_labels)\n\n# Squeeze and Excitation Block\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n# Feature Pyramid Network (FPN) module\nclass FPN(nn.Module):\n    def __init__(self, channels):\n        super(FPN, self).__init__()\n        self.lateral_conv = nn.Conv2d(channels, 256, kernel_size=1)\n        self.output_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n       \n    def forward(self, x):\n        lateral = self.lateral_conv(x)\n        output = self.output_conv(lateral)\n        return output\n\n# Transformer-based attention block\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads=4, mlp_ratio=4.0, qkv_bias=True, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, int(dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(dim * mlp_ratio), dim),\n            nn.Dropout(dropout)\n        )\n       \n    def forward(self, x):\n        # Reshape to sequence for attention\n        b, c, h, w = x.shape\n        x_seq = x.flatten(2).transpose(1, 2)  # B, HW, C\n       \n        # Apply normalization\n        x_norm = self.norm1(x_seq)\n       \n        # Self-attention\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x_seq = x_seq + attn_out\n       \n        # MLP block\n        x_seq = x_seq + self.mlp(self.norm2(x_seq))\n       \n        # Reshape back to feature map\n        x = x_seq.transpose(1, 2).reshape(b, c, h, w)\n       \n        return x\n\n# Coordinate Attention\nclass CoordinateAttention(nn.Module):\n    def __init__(self, inp, oup, reduction=32):\n        super(CoordinateAttention, self).__init__()\n        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n       \n        mip = max(8, inp // reduction)\n       \n        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n        self.bn1 = nn.BatchNorm2d(mip)\n        self.act = nn.ReLU(inplace=True)\n       \n        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n       \n    def forward(self, x):\n        identity = x\n       \n        n, c, h, w = x.size()\n        x_h = self.pool_h(x)  # n,c,h,1\n        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # n,c,1,w -> n,c,w,1\n       \n        y = torch.cat([x_h, x_w], dim=2)\n        y = self.conv1(y)\n        y = self.bn1(y)\n        y = self.act(y)\n       \n        x_h, x_w = torch.split(y, [h, w], dim=2)\n        x_w = x_w.permute(0, 1, 3, 2)\n       \n        a_h = self.conv_h(x_h).sigmoid()\n        a_w = self.conv_w(x_w).sigmoid()\n       \n        out = identity * a_h * a_w\n       \n        return out\n\n# Main model with Transformer-Enhanced Attention\nclass TransformerBrainTumorClassifier(nn.Module):\n    def __init__(self, num_classes=3):\n        super(TransformerBrainTumorClassifier, self).__init__()\n       \n        # Load pretrained DenseNet and remove final classifier\n        densenet = models.densenet121(weights=\"IMAGENET1K_V1\")\n        self.features = nn.Sequential(*list(densenet.features.children()))\n       \n        # Feature channels at different levels (assuming 1024 total features from DenseNet121)\n        feature_dim = 1024\n        split_dim = feature_dim // 2\n       \n        # Replace standard attention with transformer blocks\n        self.transformer1 = TransformerBlock(split_dim, num_heads=4, dropout=0.1)\n        self.transformer2 = TransformerBlock(split_dim, num_heads=4, dropout=0.1)\n       \n        # Add coordinate attention\n        self.coord_attn1 = CoordinateAttention(split_dim, split_dim)\n        self.coord_attn2 = CoordinateAttention(split_dim, split_dim)\n       \n        # Keep SE blocks\n        self.se1 = SEBlock(split_dim)\n        self.se2 = SEBlock(split_dim)\n       \n        # FPN modules\n        self.fpn1 = FPN(split_dim)\n        self.fpn2 = FPN(split_dim)\n       \n        # Global pooling and classifier\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n       \n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n       \n    def forward(self, x):\n        # Extract features using DenseNet backbone\n        features = self.features(x)\n       \n        # Split features into two parts\n        features1, features2 = torch.split(features, features.size(1)//2, dim=1)\n       \n        # Apply transformer attention\n        features1 = self.transformer1(features1)\n        features2 = self.transformer2(features2)\n       \n        # Apply coordinate attention\n        features1 = self.coord_attn1(features1)\n        features2 = self.coord_attn2(features2)\n       \n        # Apply SE blocks\n        features1 = self.se1(features1)\n        features2 = self.se2(features2)\n       \n        # Apply FPN\n        features1 = self.fpn1(features1)\n        features2 = self.fpn2(features2)\n       \n        # Concatenate features\n        combined_features = torch.cat([features1, features2], dim=1)\n       \n        # Global pooling\n        pooled = self.global_pool(combined_features)\n        pooled = pooled.view(pooled.size(0), -1)\n       \n        # Classification\n        output = self.classifier(pooled)\n       \n        return output\n\n# Training function with 10 epochs\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n    best_val_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n   \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n       \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n       \n        for inputs, labels in train_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n           \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n           \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n           \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n           \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n       \n        scheduler.step()\n       \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n       \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.cpu().numpy())\n       \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n       \n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        val_running_corrects = 0\n       \n        # No gradient calculation needed for validation\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n               \n                # Forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n               \n                # Statistics\n                val_running_loss += loss.item() * inputs.size(0)\n                val_running_corrects += torch.sum(preds == labels.data)\n       \n        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n        val_epoch_acc = val_running_corrects.double() / len(val_loader.dataset)\n       \n        history['val_loss'].append(val_epoch_loss)\n        history['val_acc'].append(val_epoch_acc.cpu().numpy())\n       \n        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n       \n        # Save best model\n        if val_epoch_acc > best_val_acc:\n            best_val_acc = val_epoch_acc\n            torch.save(model.state_dict(), 'best_transformer_model.pth')\n            print(f'New best model saved with accuracy: {val_epoch_acc:.4f}')\n       \n        print()\n   \n    # Load best model weights\n    model.load_state_dict(torch.load('best_transformer_model.pth'))\n    return model, history\n\n# Evaluation function with advanced metrics\ndef evaluate_model(model, test_loader, num_classes=3):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n   \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = F.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n           \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n   \n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n   \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n   \n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n   \n    # Classification Report\n    report = classification_report(all_labels, all_preds)\n   \n    # ROC Curve and AUC\n    # Binarize the labels for ROC calculation\n    labels_bin = label_binarize(all_labels, classes=range(num_classes))\n   \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(num_classes):\n        fpr[i], tpr[i], _ = roc_curve(labels_bin[:, i], all_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n   \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_bin.ravel(), all_probs.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n   \n    # Compute macro-average ROC curve and ROC area\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n   \n    # Then interpolate all ROC curves at these points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(num_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n   \n    # Finally average it and compute AUC\n    mean_tpr /= num_classes\n   \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n   \n    # Plot ROC Curves\n    plt.figure(figsize=(12, 8))\n   \n    # Plot micro-average ROC curve\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n             color='deeppink', linestyle=':', linewidth=4)\n   \n    # Plot macro-average ROC curve\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=f'Macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n             color='navy', linestyle=':', linewidth=4)\n   \n    # Plot ROC curves for all classes\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n   \n    for i, color, name in zip(range(num_classes), colors, class_names):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label=f'ROC curve of {name} (area = {roc_auc[i]:.2f})')\n   \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('transformer_roc_curve.png')\n    plt.close()\n   \n    # Plot Confusion Matrix\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n   \n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n   \n    # Add text annotations to the confusion matrix\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], 'd'),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n   \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig('transformer_confusion_matrix.png')\n    plt.close()\n   \n    # Return metrics and predictions\n    return {\n        'accuracy': accuracy,\n        'confusion_matrix': cm,\n        'classification_report': report,\n        'roc_auc': roc_auc,\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs\n    }\n\n# Function to test with a random image\ndef test_random_image(model, images, labels, transform=None):\n    \"\"\"\n    Test the model with a random image from the dataset\n   \n    Args:\n        model: Trained model\n        images: Image data\n        labels: Labels\n        transform: Image transform function\n       \n    Returns:\n        prediction: The model's prediction\n    \"\"\"\n    # Choose a random image\n    idx = random.randint(0, len(images) - 1)\n    img = images[idx]\n    true_label = labels[idx] - 1  # Convert to 0-indexed\n   \n    # Preprocess the image\n    img = np.repeat(img.reshape(224, 224, 1), 3, axis=2)\n   \n    # Apply transform if provided\n    if transform:\n        img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n    else:\n        img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n   \n    # Get model prediction\n    model.eval()\n    with torch.no_grad():\n        output = model(img_tensor)\n        probs = F.softmax(output, dim=1)\n        _, pred = torch.max(output, 1)\n   \n    # Display the image and prediction\n    class_names = ['Meningioma', 'Glioma', 'Pituitary']\n    plt.figure(figsize=(6, 6))\n    plt.imshow(img, cmap='gray')\n    plt.title(f'True: {class_names[true_label]}, Pred: {class_names[pred.item()]}\\nConfidence: {probs[0][pred.item()]:.4f}')\n    plt.axis('off')\n    plt.savefig('transformer_random_test.png')\n    plt.close()\n   \n    print(f\"Random image test - True: {class_names[true_label]}, Predicted: {class_names[pred.item()]}\")\n    print(f\"Confidence scores: {probs[0].cpu().numpy()}\")\n   \n    return {\n        'image_idx': idx,\n        'true_label': true_label,\n        'predicted_label': pred.item(),\n        'confidence': probs[0][pred.item()].item(),\n        'all_probs': probs[0].cpu().numpy()\n    }\n\n# Main execution function\ndef run_experiment(fold_index):\n    print(f\"\\n{'='*20} RUNNING FOLD {fold_index} {'='*20}\\n\")\n   \n    # Load data\n    images, labels, fold_indices = load_data()\n   \n    # Get train and test data for this fold\n    (train_images, train_labels), (test_images, test_labels) = get_train_test_data(\n        images, labels, fold_indices, fold_index)\n   \n    # Create datasets with transforms\n    train_transform, val_transform = get_transforms()\n   \n    train_dataset = BrainTumorDataset(train_images, train_labels, transform=train_transform)\n    test_dataset = BrainTumorDataset(test_images, test_labels, transform=val_transform)\n   \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n   \n    # Initialize model with Transformer-Enhanced architecture\n    model = TransformerBrainTumorClassifier(num_classes=3)\n    model = model.to(device)\n   \n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n   \n    # Set scheduler with T_max=10 to match epochs\n    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n   \n    # Train model with 10 epochs\n    start_time = time.time()\n    model, history = train_model(\n        model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=10)\n    end_time = time.time()\n   \n    # Evaluate model with advanced metrics\n    metrics = evaluate_model(model, test_loader)\n    accuracy = metrics['accuracy']\n   \n    print(f\"Fold {fold_index} accuracy: {accuracy:.4f}\")\n    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n    print(\"\\nClassification Report:\")\n    print(metrics['classification_report'])\n   \n    # Get ROC AUC scores\n    print(\"\\nROC AUC Scores:\")\n    for i in range(3):\n        print(f\"Class {i}: {metrics['roc_auc'][i]:.4f}\")\n    print(f\"Micro-average: {metrics['roc_auc']['micro']:.4f}\")\n    print(f\"Macro-average: {metrics['roc_auc']['macro']:.4f}\")\n   \n    # Plot loss curve\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Loss Curves for Fold {fold_index} with Transformer')\n    plt.savefig(f'transformer_loss_curve_fold_{fold_index}.png')\n    plt.close()\n   \n    # Test with a random image\n    random_test_results = test_random_image(model, images, labels, transform=val_transform)\n   \n    # Save model and results\n    torch.save(model.state_dict(), f'transformer_model_fold_{fold_index}.pth')\n    np.save(f'transformer_results_fold_{fold_index}.npy', metrics)\n   \n    return metrics\n\n# Run for all folds\nif __name__ == \"__main__\":\n    results_all = {}\n   \n    # Run for all 5 folds\n    for fold in range(1, 6):\n        try:\n            metrics = run_experiment(fold)\n            results_all[fold] = metrics\n        except Exception as e:\n            print(f\"Error in fold {fold}: {e}\")\n   \n    # Calculate and print average accuracy across all folds\n    accuracies = [results_all[fold]['accuracy'] for fold in results_all]\n    if accuracies:\n        avg_accuracy = np.mean(accuracies)\n        print(f\"\\nAverage accuracy across all folds: {avg_accuracy:.4f}\")\n       \n        # Calculate average AUC across all folds\n        avg_auc_micro = np.mean([results_all[fold]['roc_auc']['micro'] for fold in results_all])\n        avg_auc_macro = np.mean([results_all[fold]['roc_auc']['macro'] for fold in results_all])\n        print(f\"Average micro-average AUC: {avg_auc_micro:.4f}\")\n        print(f\"Average macro-average AUC: {avg_auc_macro:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:31:39.830456Z","iopub.execute_input":"2025-05-01T15:31:39.831075Z","iopub.status.idle":"2025-05-01T15:57:04.799989Z","shell.execute_reply.started":"2025-05-01T15:31:39.831051Z","shell.execute_reply":"2025-05-01T15:57:04.799023Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDownloading dataset...\nPath to dataset files: /kaggle/input/figshare-brain-tumor-dataset\n\n==================== RUNNING FOLD 1 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 153MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n----------\nTrain Loss: 0.3406 Acc: 0.8676\nVal Loss: 0.1878 Acc: 0.9354\nNew best model saved with accuracy: 0.9354\n\nEpoch 2/10\n----------\nTrain Loss: 0.1984 Acc: 0.9306\nVal Loss: 0.1952 Acc: 0.9299\n\nEpoch 3/10\n----------\nTrain Loss: 0.1584 Acc: 0.9500\nVal Loss: 0.0787 Acc: 0.9760\nNew best model saved with accuracy: 0.9760\n\nEpoch 4/10\n----------\nTrain Loss: 0.0994 Acc: 0.9651\nVal Loss: 0.1057 Acc: 0.9649\n\nEpoch 5/10\n----------\nTrain Loss: 0.0853 Acc: 0.9715\nVal Loss: 0.0992 Acc: 0.9668\n\nEpoch 6/10\n----------\nTrain Loss: 0.0672 Acc: 0.9802\nVal Loss: 0.0758 Acc: 0.9723\n\nEpoch 7/10\n----------\nTrain Loss: 0.0697 Acc: 0.9806\nVal Loss: 0.0734 Acc: 0.9760\n\nEpoch 8/10\n----------\nTrain Loss: 0.0353 Acc: 0.9913\nVal Loss: 0.0617 Acc: 0.9760\n\nEpoch 9/10\n----------\nTrain Loss: 0.0264 Acc: 0.9941\nVal Loss: 0.0526 Acc: 0.9797\nNew best model saved with accuracy: 0.9797\n\nEpoch 10/10\n----------\nTrain Loss: 0.0254 Acc: 0.9925\nVal Loss: 0.0518 Acc: 0.9834\nNew best model saved with accuracy: 0.9834\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:404: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_transformer_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 accuracy: 0.9834\nTraining time: 308.33 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       113\n           1       0.99      0.98      0.99       288\n           2       0.99      1.00      0.99       141\n\n    accuracy                           0.98       542\n   macro avg       0.98      0.98      0.98       542\nweighted avg       0.98      0.98      0.98       542\n\n\nROC AUC Scores:\nClass 0: 0.9981\nClass 1: 0.9997\nClass 2: 0.9998\nMicro-average: 0.9993\nMacro-average: 0.9993\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [0.00108965 0.00114193 0.9977684 ]\n\n==================== RUNNING FOLD 2 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/10\n----------\nError in fold 2: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n\n==================== RUNNING FOLD 3 ====================\n\nFound 3064 .mat files\nProcessed 0 images\nProcessed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/10\n----------\nTrain Loss: 0.3518 Acc: 0.8684\nVal Loss: 0.1603 Acc: 0.9301\nNew best model saved with accuracy: 0.9301\n\nEpoch 2/10\n----------\nTrain Loss: 0.1940 Acc: 0.9306\nVal Loss: 0.1621 Acc: 0.9458\nNew best model saved with accuracy: 0.9458\n\nEpoch 3/10\n----------\nTrain Loss: 0.1355 Acc: 0.9571\nVal Loss: 0.0859 Acc: 0.9668\nNew best model saved with accuracy: 0.9668\n\nEpoch 4/10\n----------\nTrain Loss: 0.1065 Acc: 0.9667\nVal Loss: 0.1445 Acc: 0.9528\n\nEpoch 5/10\n----------\nTrain Loss: 0.0956 Acc: 0.9695\nVal Loss: 0.0582 Acc: 0.9843\nNew best model saved with accuracy: 0.9843\n\nEpoch 6/10\n----------\nTrain Loss: 0.0692 Acc: 0.9743\nVal Loss: 0.0579 Acc: 0.9878\nNew best model saved with accuracy: 0.9878\n\nEpoch 7/10\n----------\nTrain Loss: 0.0452 Acc: 0.9884\nVal Loss: 0.0453 Acc: 0.9895\nNew best model saved with accuracy: 0.9895\n\nEpoch 8/10\n----------\nTrain Loss: 0.0471 Acc: 0.9852\nVal Loss: 0.0583 Acc: 0.9860\n\nEpoch 9/10\n----------\nTrain Loss: 0.0331 Acc: 0.9916\nVal Loss: 0.0435 Acc: 0.9895\n\nEpoch 10/10\n----------\nTrain Loss: 0.0230 Acc: 0.9952\nVal Loss: 0.0434 Acc: 0.9895\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:404: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_transformer_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 accuracy: 0.9895\nTraining time: 310.64 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97        99\n           1       1.00      0.99      0.99       267\n           2       1.00      0.99      1.00       206\n\n    accuracy                           0.99       572\n   macro avg       0.98      0.99      0.99       572\nweighted avg       0.99      0.99      0.99       572\n\n\nROC AUC Scores:\nClass 0: 0.9981\nClass 1: 0.9975\nClass 2: 1.0000\nMicro-average: 0.9978\nMacro-average: 0.9992\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [4.5212326e-04 1.8040824e-03 9.9774384e-01]\n\n==================== RUNNING FOLD 4 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/10\n----------\nTrain Loss: 0.3231 Acc: 0.8773\nVal Loss: 0.2495 Acc: 0.8901\nNew best model saved with accuracy: 0.8901\n\nEpoch 2/10\n----------\nTrain Loss: 0.1883 Acc: 0.9368\nVal Loss: 0.1428 Acc: 0.9475\nNew best model saved with accuracy: 0.9475\n\nEpoch 3/10\n----------\nTrain Loss: 0.1473 Acc: 0.9516\nVal Loss: 0.1012 Acc: 0.9666\nNew best model saved with accuracy: 0.9666\n\nEpoch 4/10\n----------\nTrain Loss: 0.1136 Acc: 0.9622\nVal Loss: 0.1085 Acc: 0.9634\n\nEpoch 5/10\n----------\nTrain Loss: 0.0831 Acc: 0.9741\nVal Loss: 0.0879 Acc: 0.9713\nNew best model saved with accuracy: 0.9713\n\nEpoch 6/10\n----------\nTrain Loss: 0.0668 Acc: 0.9782\nVal Loss: 0.0993 Acc: 0.9650\n\nEpoch 7/10\n----------\nTrain Loss: 0.0546 Acc: 0.9852\nVal Loss: 0.0841 Acc: 0.9697\n\nEpoch 8/10\n----------\nTrain Loss: 0.0407 Acc: 0.9877\nVal Loss: 0.0617 Acc: 0.9793\nNew best model saved with accuracy: 0.9793\n\nEpoch 9/10\n----------\nTrain Loss: 0.0375 Acc: 0.9869\nVal Loss: 0.0508 Acc: 0.9793\n\nEpoch 10/10\n----------\nTrain Loss: 0.0272 Acc: 0.9922\nVal Loss: 0.0487 Acc: 0.9809\nNew best model saved with accuracy: 0.9809\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:404: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_transformer_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 accuracy: 0.9809\nTraining time: 307.96 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       168\n           1       0.98      0.98      0.98       287\n           2       0.98      1.00      0.99       173\n\n    accuracy                           0.98       628\n   macro avg       0.98      0.98      0.98       628\nweighted avg       0.98      0.98      0.98       628\n\n\nROC AUC Scores:\nClass 0: 0.9988\nClass 1: 0.9992\nClass 2: 0.9999\nMicro-average: 0.9994\nMacro-average: 0.9994\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [1.6409383e-04 5.1482652e-06 9.9983072e-01]\n\n==================== RUNNING FOLD 5 ====================\n\nFound 3064 .mat files\nProcessed 0 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  lbl.append(int(images['label'][0]))\n","output_type":"stream"},{"name":"stdout","text":"Processed 500 images\nProcessed 1000 images\nProcessed 1500 images\nProcessed 2000 images\nProcessed 2500 images\nProcessed 3000 images\nFound cvind.mat at: /kaggle/input/figshare-brain-tumor-dataset/dataset/cvind.mat\nEpoch 1/10\n----------\nTrain Loss: 0.3352 Acc: 0.8769\nVal Loss: 0.2275 Acc: 0.9176\nNew best model saved with accuracy: 0.9176\n\nEpoch 2/10\n----------\nTrain Loss: 0.1955 Acc: 0.9294\nVal Loss: 0.1333 Acc: 0.9565\nNew best model saved with accuracy: 0.9565\n\nEpoch 3/10\n----------\nTrain Loss: 0.1655 Acc: 0.9434\nVal Loss: 0.1275 Acc: 0.9549\n\nEpoch 4/10\n----------\nTrain Loss: 0.1004 Acc: 0.9632\nVal Loss: 0.1173 Acc: 0.9627\nNew best model saved with accuracy: 0.9627\n\nEpoch 5/10\n----------\nTrain Loss: 0.0770 Acc: 0.9773\nVal Loss: 0.1285 Acc: 0.9502\n\nEpoch 6/10\n----------\nTrain Loss: 0.0600 Acc: 0.9806\nVal Loss: 0.0985 Acc: 0.9689\nNew best model saved with accuracy: 0.9689\n\nEpoch 7/10\n----------\nTrain Loss: 0.0665 Acc: 0.9831\nVal Loss: 0.0665 Acc: 0.9736\nNew best model saved with accuracy: 0.9736\n\nEpoch 8/10\n----------\nTrain Loss: 0.0447 Acc: 0.9855\nVal Loss: 0.0689 Acc: 0.9782\nNew best model saved with accuracy: 0.9782\n\nEpoch 9/10\n----------\nTrain Loss: 0.0291 Acc: 0.9922\nVal Loss: 0.0697 Acc: 0.9751\n\nEpoch 10/10\n----------\nTrain Loss: 0.0324 Acc: 0.9905\nVal Loss: 0.0670 Acc: 0.9767\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/321735910.py:404: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_transformer_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 accuracy: 0.9782\nTraining time: 306.54 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96       144\n           1       0.99      0.98      0.99       296\n           2       0.98      0.99      0.98       203\n\n    accuracy                           0.98       643\n   macro avg       0.97      0.98      0.97       643\nweighted avg       0.98      0.98      0.98       643\n\n\nROC AUC Scores:\nClass 0: 0.9965\nClass 1: 0.9995\nClass 2: 0.9985\nMicro-average: 0.9985\nMacro-average: 0.9985\nRandom image test - True: Pituitary, Predicted: Pituitary\nConfidence scores: [8.1296668e-05 3.9397471e-04 9.9952483e-01]\n\nAverage accuracy across all folds: 0.9830\nAverage micro-average AUC: 0.9987\nAverage macro-average AUC: 0.9991\n","output_type":"stream"}],"execution_count":1}]}